{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Imports**",
   "id": "22c7c720151f512c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T15:47:18.121649Z",
     "start_time": "2025-09-01T15:47:18.115655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.OrbitPlotter import compare_orbits\n",
    "from utils.TleUtils import (\n",
    "    parse_norad_from_tle1,\n",
    "    parse_line2_params,\n",
    ")\n",
    "from utils.DataImporter import (\n",
    "fetch_celestrak_debris,\n",
    "fetch_spacetrack_sets\n",
    ")\n",
    "from APIs.SpaceTrackAPI import space_track_client, SpaceTrackAuthError"
   ],
   "id": "f82e34596e8ecd1b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Fetch data from APIs",
   "id": "184074aaeb73f2c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fetch_spacetrack_sets()\n",
    "fetch_celestrak_debris()"
   ],
   "id": "889b185905b31aa0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Load all Data from DATA repository**"
   ],
   "id": "599f26d3752a9c18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pre-process datasets",
   "id": "8c2a0e5e004c7dff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_dir = \"DATA\"\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "dataframes = {}\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    dataframes[file] = df\n",
    "\n",
    "# Preview\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(df.head())\n"
   ],
   "id": "56cd9ccf3884f123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DATE_COLS = {\"EPOCH\", \"LAUNCH\", \"DECAY_EPOCH\", \"TCA\", \"CREATED\", \"DECAY\", \"TLE_EPOCH\"}\n",
    "\n",
    "def parse_tle_epoch(line1: str):\n",
    "    \"\"\"\n",
    "    Parse epoch year (YY) and day of year (DDDDDD.DDDDD) from TLE line 1.\n",
    "    Returns (epoch_year, epoch_day) as (int or None, float or None).\n",
    "    Columns (0-based slices): year [18:20], day [20:32].\n",
    "    \"\"\"\n",
    "    if not isinstance(line1, str) or len(line1) < 32:\n",
    "        return None, None\n",
    "    try:\n",
    "        yy = line1[18:20]\n",
    "        day = line1[20:32]\n",
    "        epoch_year = int(yy)\n",
    "        # Convert 2-digit year to full year (assume 1957-2056 per TLE convention)\n",
    "        full_year = 1900 + epoch_year if epoch_year >= 57 else 2000 + epoch_year\n",
    "        epoch_day = float(day.strip()) if day.strip() else None\n",
    "        return full_year, epoch_day\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def parse_line2_more(line2: str):\n",
    "    \"\"\"\n",
    "    Parse RAAN, Eccentricity, Arg of Perigee, Mean Anomaly from TLE line 2.\n",
    "    Fixed columns (0-based):\n",
    "      RAAN [17:25]\n",
    "      Ecc  [26:33] (implied decimal: 0.x)\n",
    "      ArgP [34:42]\n",
    "      MeanA[43:51]\n",
    "    Returns (raan_deg, ecc, argp_deg, meana_deg)\n",
    "    \"\"\"\n",
    "    if not isinstance(line2, str) or len(line2) < 51:\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        raan = float(line2[17:25].strip()) if line2[17:25].strip() else None\n",
    "        ecc_str = line2[26:33].strip()\n",
    "        ecc = float(f\"0.{ecc_str}\") if ecc_str else None\n",
    "        argp = float(line2[34:42].strip()) if line2[34:42].strip() else None\n",
    "        meana = float(line2[43:51].strip()) if line2[43:51].strip() else None\n",
    "        return raan, ecc, argp, meana\n",
    "    except Exception:\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def ensure_datetime(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if col.upper() in DATE_COLS:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            # Skip columns with TLE lines or IDs that are clearly non-numeric\n",
    "            if col.upper() in {\"LINE1\", \"LINE2\", \"TLE_LINE1\", \"TLE_LINE2\", \"OBJECT_ID\"}:\n",
    "                continue\n",
    "            # Try numeric conversion\n",
    "            converted = pd.to_numeric(df[col], errors=\"ignore\")\n",
    "            df[col] = converted\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardize_ids(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create/standardize NORAD_CAT_ID column as integer where possible.\n",
    "    Sources: NORAD_CAT_ID, OBJECT_NUMBER, OBJECT_ID, or parsed from TLE line1.\n",
    "    \"\"\"\n",
    "    norad = None\n",
    "    # Start from existing NORAD_CAT_ID or OBJECT_NUMBER\n",
    "    for candidate in [\n",
    "        \"NORAD_CAT_ID\", \"OBJECT_NUMBER\", \"NORAD_ID\", \"CATNR\", \"NORAD\",\n",
    "    ]:\n",
    "        if candidate in df.columns:\n",
    "            try:\n",
    "                temp = pd.to_numeric(df[candidate], errors=\"coerce\").astype(\"Int64\")\n",
    "                norad = temp if norad is None else norad.fillna(temp)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # OBJECT_ID sometimes contains the Launch ID, not NORAD; do not parse number from it directly.\n",
    "\n",
    "    # Parse from TLE line1 where available\n",
    "    line1_col = \"LINE1\" if \"LINE1\" in df.columns else (\"TLE_LINE1\" if \"TLE_LINE1\" in df.columns else None)\n",
    "    if line1_col:\n",
    "        parsed = df[line1_col].apply(lambda x: parse_norad_from_tle1(x) if isinstance(x, str) else None)\n",
    "        parsed = pd.Series(parsed, index=df.index, dtype=\"float\").astype(\"Int64\")\n",
    "        norad = parsed if norad is None else norad.fillna(parsed)\n",
    "\n",
    "    if norad is not None:\n",
    "        df[\"NORAD_CAT_ID_STD\"] = norad\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_tle_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If TLE columns exist, parse and add key orbital parameters.\n",
    "    \"\"\"\n",
    "    line1_col = None\n",
    "    line2_col = None\n",
    "    # Detect variants\n",
    "    if \"line1\" in df.columns and \"line2\" in df.columns:\n",
    "        line1_col, line2_col = \"line1\", \"line2\"\n",
    "    elif \"LINE1\" in df.columns and \"LINE2\" in df.columns:\n",
    "        line1_col, line2_col = \"LINE1\", \"LINE2\"\n",
    "    elif \"TLE_LINE1\" in df.columns and \"TLE_LINE2\" in df.columns:\n",
    "        line1_col, line2_col = \"TLE_LINE1\", \"TLE_LINE2\"\n",
    "\n",
    "    if line1_col and line2_col:\n",
    "        # Epoch Year/Day\n",
    "        epochs = df[line1_col].apply(lambda x: parse_tle_epoch(x))\n",
    "        df[\"TLE_EPOCH_YEAR\"] = [e[0] for e in epochs]\n",
    "        df[\"TLE_EPOCH_DOY\"] = [e[1] for e in epochs]\n",
    "        # Inclination and Mean Motion from utils\n",
    "        inc_mm = df[line2_col].apply(lambda x: parse_line2_params(x))\n",
    "        df[\"INCLINATION_DEG\"] = [p[0] for p in inc_mm]\n",
    "        df[\"MEAN_MOTION_REV_PER_DAY\"] = [p[1] for p in inc_mm]\n",
    "        # More params\n",
    "        raan_ecc_argp_meana = df[line2_col].apply(lambda x: parse_line2_more(x))\n",
    "        df[\"RAAN_DEG\"] = [p[0] for p in raan_ecc_argp_meana]\n",
    "        df[\"ECCENTRICITY\"] = [p[1] for p in raan_ecc_argp_meana]\n",
    "        df[\"ARG_OF_PERIGEE_DEG\"] = [p[2] for p in raan_ecc_argp_meana]\n",
    "        df[\"MEAN_ANOMALY_DEG\"] = [p[3] for p in raan_ecc_argp_meana]\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # DECAY missing -> informative: not decayed\n",
    "    if \"DECAY\" in df.columns:\n",
    "        df[\"DECAY\"] = pd.to_datetime(df[\"DECAY\"], errors=\"coerce\", utc=True)\n",
    "        df[\"HAS_DECAYED\"] = df[\"DECAY\"].notna()\n",
    "    empty_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "    if empty_cols:\n",
    "        df = df.drop(columns=empty_cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "preprocessed = {}\n",
    "for name, df in dataframes.items():\n",
    "    df2 = df.copy()\n",
    "    df2 = ensure_datetime(df2)\n",
    "    df2 = ensure_numeric(df2)\n",
    "    df2 = parse_tle_columns(df2)\n",
    "    df2 = standardize_ids(df2)\n",
    "    df2 = handle_missing_values(df2)\n",
    "    clean_name = name.replace(\".csv\", \"\")\n",
    "    preprocessed[clean_name] = df2\n",
    "\n",
    "\n",
    "# 1. Join TLE + SATCAT (orbital params + metadata)\n",
    "tle_satcat = preprocessed['spacetrack_tle_latest'].merge(\n",
    "    preprocessed['spacetrack_satcat'],\n",
    "    on=\"NORAD_CAT_ID_STD\",\n",
    "    suffixes=(\"_tle\", \"_satcat\")\n",
    ")\n",
    "\n",
    "# 2. Join SATCAT + Decay (lifetime info)\n",
    "satcat_decay = preprocessed['spacetrack_satcat'].merge(\n",
    "    preprocessed['spacetrack_decay_5y'],\n",
    "    on=\"NORAD_CAT_ID_STD\",\n",
    "    suffixes=(\"_satcat\", \"_decay\")\n",
    ")\n",
    "# 3. Join CDM + SATCAT (collision warnings + metadata)\n",
    "cdm_sat1 = preprocessed['spacetrack_cdm_public_30d'].merge(\n",
    "    preprocessed['spacetrack_satcat'],\n",
    "    left_on=\"SAT_1_ID\",\n",
    "    right_on=\"NORAD_CAT_ID\",\n",
    "    suffixes=(\"\", \"_SAT1\")\n",
    ")\n",
    "cdm_sat2 = preprocessed['spacetrack_cdm_public_30d'].merge(\n",
    "    preprocessed['spacetrack_satcat'],\n",
    "    left_on=\"SAT_2_ID\",\n",
    "    right_on=\"NORAD_CAT_ID\",\n",
    "    suffixes=(\"\", \"_SAT2\")\n",
    ")\n",
    "cdm_full = cdm_sat1.merge(\n",
    "    preprocessed['spacetrack_satcat'],\n",
    "    left_on=\"SAT_2_ID\",\n",
    "    right_on=\"NORAD_CAT_ID\",\n",
    "    suffixes=(\"_SAT1\", \"_SAT2\")\n",
    ")\n",
    "\n",
    "preprocessed['tle_satcat'] = tle_satcat\n",
    "preprocessed['satcat_decay'] = satcat_decay\n",
    "preprocessed['cdm_full'] = cdm_full\n",
    "\n",
    "print(\"\\nPreprocessing complete\")\n",
    "for name, df in preprocessed.items():\n",
    "    cols = [c for c in [\n",
    "        \"NORAD_CAT_ID_STD\",\n",
    "        \"TLE_EPOCH_YEAR\", \"TLE_EPOCH_DOY\",\n",
    "        \"INCLINATION_DEG\", \"RAAN_DEG\", \"ECCENTRICITY\",\n",
    "        \"ARG_OF_PERIGEE_DEG\", \"MEAN_ANOMALY_DEG\", \"MEAN_MOTION_REV_PER_DAY\",\n",
    "        \"HAS_DECAYED\",\n",
    "    ] if c in df.columns]\n",
    "    print(f\"- {name}: columns -> {cols[:10]}\")\n",
    "    print(df[cols].head() if cols else df.head())\n"
   ],
   "id": "7030420b80909e83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Univariate Analysis**",
   "id": "ab06004bcbb519c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "MU_EARTH_KM3_S2 = 398600.4418  # km^3 / s^2\n",
    "R_EARTH_KM = 6378.137          # km (equatorial radius)\n",
    "SECONDS_PER_DAY = 86400.0\n",
    "TWOPI = 2.0 * np.pi\n",
    "\n",
    "\n",
    "def add_orbital_derived_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Using MEAN_MOTION_REV_PER_DAY (n, rev/day) and ECCENTRICITY (e),\n",
    "    compute:\n",
    "      - PERIOD_MIN (orbital period in minutes)\n",
    "      - SEMIMAJOR_AXIS_KM (a)\n",
    "      - PERIGEE_ALT_KM and APOGEE_ALT_KM (altitudes above Earth's surface)\n",
    "    Only creates columns when inputs are present; leaves existing data untouched otherwise.\n",
    "    \"\"\"\n",
    "    if not {\"MEAN_MOTION_REV_PER_DAY\", \"ECCENTRICITY\"}.issubset(df.columns):\n",
    "        return df\n",
    "\n",
    "    n = pd.to_numeric(df[\"MEAN_MOTION_REV_PER_DAY\"], errors=\"coerce\")\n",
    "    e = pd.to_numeric(df[\"ECCENTRICITY\"], errors=\"coerce\")\n",
    "\n",
    "    # Period in minutes (1440 minutes per day)\n",
    "    period_min = 1440.0 / n\n",
    "\n",
    "    # Convert mean motion rev/day to rad/s\n",
    "    n_rad_s = (n * TWOPI) / SECONDS_PER_DAY\n",
    "    # Semi-major axis from n: a = (mu / n^2)^(1/3)\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        a_km = (MU_EARTH_KM3_S2 / (n_rad_s ** 2)) ** (1.0 / 3.0)\n",
    "\n",
    "    # Perigee/Apogee radii and altitudes\n",
    "    r_p_km = a_km * (1.0 - e)\n",
    "    r_a_km = a_km * (1.0 + e)\n",
    "    perigee_alt_km = r_p_km - R_EARTH_KM\n",
    "    apogee_alt_km = r_a_km - R_EARTH_KM\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"PERIOD_MIN\"] = period_min\n",
    "    df[\"SEMIMAJOR_AXIS_KM\"] = a_km\n",
    "    df[\"PERIGEE_ALT_KM\"] = perigee_alt_km\n",
    "    df[\"APOGEE_ALT_KM\"] = apogee_alt_km\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_hist(ax, series, title, xlabel, bins=60, log_scale=False, xlim=None):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        return\n",
    "    if log_scale:\n",
    "        s = s[s > 0]\n",
    "    sns.histplot(s, bins=bins, ax=ax, kde=False, edgecolor=None)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    if log_scale:\n",
    "        ax.set_xscale('log')\n",
    "    if xlim is not None:\n",
    "        ax.set_xlim(xlim)\n",
    "\n",
    "\n",
    "for name, df in preprocessed.items():\n",
    "    preprocessed[name] = add_orbital_derived_columns(df)\n",
    "\n",
    "print(\"\\nDerived orbital parameters added (where available): PERIOD_MIN, SEMIMAJOR_AXIS_KM, PERIGEE_ALT_KM, APOGEE_ALT_KM\")\n",
    "\n",
    "# Identify datasets that contain TLE-derived columns\n",
    "datasets_with_tle = []\n",
    "for name, df in preprocessed.items():\n",
    "    needed = {\"INCLINATION_DEG\", \"ECCENTRICITY\", \"MEAN_MOTION_REV_PER_DAY\"}\n",
    "    if needed.issubset(df.columns):\n",
    "        datasets_with_tle.append((name, df))\n",
    "\n",
    "print(\"Datasets with TLE params:\", [n for n, _ in datasets_with_tle])\n",
    "\n",
    "# Plot univariate distributions per dataset\n",
    "for name, df in datasets_with_tle:\n",
    "    print(f\"\\nPlotting univariate distributions for: {name}\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 9))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # 1) Inclination (deg)\n",
    "    plot_hist(\n",
    "        axes[0],\n",
    "        df[\"INCLINATION_DEG\"],\n",
    "        title=\"Inclination Distribution\",\n",
    "        xlabel=\"Inclination (degrees)\",\n",
    "        bins=72,  # 5-degree bins roughly\n",
    "        log_scale=False,\n",
    "        xlim=(70, 110)\n",
    "    )\n",
    "\n",
    "    # 2) Eccentricity\n",
    "    plot_hist(\n",
    "        axes[1],\n",
    "        df[\"ECCENTRICITY\"],\n",
    "        title=\"Eccentricity Distribution\",\n",
    "        xlabel=\"Eccentricity (e)\",\n",
    "        bins=60,\n",
    "        log_scale=False,\n",
    "        xlim=(0, 0.2)\n",
    "    )\n",
    "\n",
    "    perigee = pd.to_numeric(df.get(\"PERIGEE_ALT_KM\", pd.Series(dtype=float)), errors=\"coerce\")\n",
    "    apogee = pd.to_numeric(df.get(\"APOGEE_ALT_KM\", pd.Series(dtype=float)), errors=\"coerce\")\n",
    "    period = pd.to_numeric(df.get(\"PERIOD_MIN\", pd.Series(dtype=float)), errors=\"coerce\")\n",
    "\n",
    "    # 3) Perigee altitude (km)\n",
    "    plot_hist(\n",
    "        axes[2],\n",
    "        perigee[perigee >= 0],  # exclude negative/invalid altitudes\n",
    "        title=\"Perigee Altitude Distribution\",\n",
    "        xlabel=\"Perigee altitude (km)\",\n",
    "        bins=80,\n",
    "        log_scale=True,  # wide range from LEO to GEO+; log helps\n",
    "        xlim=None\n",
    "    )\n",
    "\n",
    "    # 4) Apogee altitude (km)\n",
    "    fig2, axes2 = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    plot_hist(\n",
    "        axes2[0],\n",
    "        apogee[apogee >= 0],\n",
    "        title=\"Apogee Altitude Distribution\",\n",
    "        xlabel=\"Apogee altitude (km)\",\n",
    "        bins=80,\n",
    "        log_scale=True,\n",
    "        xlim=None\n",
    "    )\n",
    "\n",
    "    # 5) Period (minutes)\n",
    "    plot_hist(\n",
    "        axes2[1],\n",
    "        period[period > 0],\n",
    "        title=\"Orbital Period Distribution\",\n",
    "        xlabel=\"Period (minutes)\",\n",
    "        bins=80,\n",
    "        log_scale=False,\n",
    "        xlim=None\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"Univariate Distributions - {name}\", y=1.02)\n",
    "    fig.tight_layout()\n",
    "    fig2.suptitle(f\"Apogee & Period - {name}\", y=1.05)\n",
    "    fig2.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "9903ddba2ec8004b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Physical Characteristics**",
   "id": "3a293d04fc2547ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _clean_categorical(series: pd.Series) -> pd.Series:\n",
    "    s = pd.Series(series)\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NONE\": np.nan})\n",
    "    s = s.fillna(\"Unknown\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def plot_bar_top(ax, series, title: str, ylabel: str, top_n: int = 20):\n",
    "    s = _clean_categorical(pd.Series(series))\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    if vc.empty:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"Count\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        return\n",
    "    if len(vc) > top_n:\n",
    "        top = vc.nlargest(top_n)\n",
    "        other_count = vc.iloc[top_n:].sum()\n",
    "        other_label = f\"Other ({len(vc) - top_n} categories)\"\n",
    "        vc_plot = pd.concat([top, pd.Series({other_label: other_count})])\n",
    "    else:\n",
    "        vc_plot = vc\n",
    "    sns.barplot(x=vc_plot.values, y=vc_plot.index, orient=\"h\", ax=ax, color=sns.color_palette()[0])\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "\n",
    "def plot_rcs(ax, df: pd.DataFrame):\n",
    "    title = \"RCS Distribution\"\n",
    "    if \"RCSVALUE\" in df.columns:\n",
    "        rcs = pd.to_numeric(df[\"RCSVALUE\"], errors=\"coerce\").dropna()\n",
    "        if len(rcs) > 0:\n",
    "            sns.histplot(rcs, bins=60, ax=ax, kde=False, edgecolor=None)\n",
    "            ax.set_title(title + \" (RCSVALUE)\")\n",
    "            ax.set_xlabel(\"RCS (m^2)\")\n",
    "            return\n",
    "    if \"RCS_SIZE\" in df.columns:\n",
    "        plot_bar_top(ax, df[\"RCS_SIZE\"], title + \" (RCS_SIZE)\", \"RCS Size\", top_n=15)\n",
    "        return\n",
    "    ax.text(0.5, 0.5, \"No RCS data\", ha=\"center\", va=\"center\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "phy_cols = {\"OBJECT_TYPE\", \"COUNTRY\", \"SITE\", \"RCSVALUE\", \"RCS_SIZE\"}\n",
    "satcat_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if any(c in df.columns for c in phy_cols):\n",
    "        satcat_datasets.append((name, df))\n",
    "\n",
    "print(\"Datasets considered for Physical Characteristics:\", [n for n, _ in satcat_datasets])\n",
    "\n",
    "# Plot Physical Characteristics per relevant dataset\n",
    "for name, df in satcat_datasets:\n",
    "    print(f\"\\nPlotting Physical Characteristics for: {name}\")\n",
    "\n",
    "    cats = [(\"OBJECT_TYPE\", \"Object Type Distribution\", \"Object Type\"),\n",
    "            (\"COUNTRY\", \"Country of Origin\", \"Country\"),\n",
    "            (\"SITE\", \"Launch Site Distribution\", \"Launch Site\")]\n",
    "    present = [(col, title, ylabel) for col, title, ylabel in cats if col in df.columns]\n",
    "\n",
    "    if present:\n",
    "        n = len(present)\n",
    "        fig, axes = plt.subplots(1, n, figsize=(6*n + 2, 5))\n",
    "        if n == 1:\n",
    "            axes = [axes]\n",
    "        for ax, (col, title, ylabel) in zip(axes, present):\n",
    "            plot_bar_top(ax, df[col], title, ylabel, top_n=15 if col != \"SITE\" else 20)\n",
    "        fig.suptitle(f\"Physical Characteristics - {name}\", y=1.02)\n",
    "        fig.tight_layout()\n",
    "\n",
    "    if (\"RCSVALUE\" in df.columns) or (\"RCS_SIZE\" in df.columns):\n",
    "        fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4))\n",
    "        plot_rcs(ax2, df)\n",
    "        fig2.suptitle(f\"RCS - {name}\", y=1.05)\n",
    "        fig2.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "c20600295bb3a333",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Temporal Analysis**",
   "id": "8926fa7f31dbf10c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _extract_year_from_datetime(series: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    return s.dt.year\n",
    "\n",
    "\n",
    "def _plot_year_hist(ax, years: pd.Series, title: str, xlabel: str):\n",
    "    y = pd.to_numeric(years, errors=\"coerce\").dropna().astype(int)\n",
    "    if y.empty:\n",
    "        ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        return\n",
    "    y_min, y_max = int(y.min()), int(y.max())\n",
    "    span = max(1, y_max - y_min + 1)\n",
    "    if span <= 120:\n",
    "        bins = span\n",
    "    else:\n",
    "        bins = 100\n",
    "    sns.histplot(y, bins=bins, ax=ax, kde=False, edgecolor=None)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "\n",
    "\n",
    "TEMPORAL_COLS = {\"LAUNCH\", \"LAUNCH_YEAR\", \"DECAY_EPOCH\", \"DECAY\"}\n",
    "temporal_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if any(c in df.columns for c in TEMPORAL_COLS):\n",
    "        temporal_datasets.append((name, df))\n",
    "\n",
    "print(\"Datasets considered for Temporal Analysis:\", [n for n, _ in temporal_datasets])\n",
    "\n",
    "for name, df in temporal_datasets:\n",
    "    print(f\"\\nPlotting Temporal Analysis for: {name}\")\n",
    "\n",
    "    launch_years = None\n",
    "    if \"LAUNCH_YEAR\" in df.columns:\n",
    "        launch_years = pd.to_numeric(df[\"LAUNCH_YEAR\"], errors=\"coerce\")\n",
    "    elif \"LAUNCH\" in df.columns:\n",
    "        launch_years = _extract_year_from_datetime(df[\"LAUNCH\"])\n",
    "\n",
    "    decay_years = None\n",
    "    if \"DECAY_EPOCH\" in df.columns:\n",
    "        decay_years = _extract_year_from_datetime(df[\"DECAY_EPOCH\"])\n",
    "    elif \"DECAY\" in df.columns:\n",
    "        decay_years = _extract_year_from_datetime(df[\"DECAY\"])  # ensure_datetime handles this too\n",
    "\n",
    "    parts = []\n",
    "    if isinstance(launch_years, pd.Series):\n",
    "        parts.append((\"Launch Year History\", \"Launch Year\", launch_years))\n",
    "    if isinstance(decay_years, pd.Series):\n",
    "        parts.append((\"Decay Epoch Distribution\", \"Decay Year\", decay_years))\n",
    "\n",
    "    if not parts:\n",
    "        print(\"  No temporal columns present; skipping.\")\n",
    "        continue\n",
    "\n",
    "    n = len(parts)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(7*n, 4.5))\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (title, xlabel, series) in zip(axes, parts):\n",
    "        _plot_year_hist(ax, series, title, xlabel)\n",
    "\n",
    "    fig.suptitle(f\"Temporal Analysis - {name}\", y=1.02)\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "7d17899726db84ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Bivariate and Multivariate Analysis**",
   "id": "6b8c8ebb429c48ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_cols = [\n",
    "    \"INCLINATION_DEG\",\n",
    "    \"ECCENTRICITY\",\n",
    "    \"PERIGEE_ALT_KM\",\n",
    "    \"APOGEE_ALT_KM\",\n",
    "]\n",
    "\n",
    "corr_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    available = [c for c in corr_cols if c in df.columns]\n",
    "    if len(available) >= 2:\n",
    "        corr_datasets.append((name, available))\n",
    "\n",
    "print(\"Datasets considered for Orbital Correlations:\", [n for n, _ in corr_datasets])\n",
    "\n",
    "for name, available in corr_datasets:\n",
    "    print(f\"\\nPlotting Correlation Heatmap for: {name}\")\n",
    "    dfnum = preprocessed[name][available].apply(pd.to_numeric, errors=\"coerce\").dropna(how=\"all\")\n",
    "    valid_cols = [c for c in dfnum.columns if not dfnum[c].isna().all()]\n",
    "    if len(valid_cols) < 2:\n",
    "        print(\"  Not enough valid numeric data; skipping.\")\n",
    "        continue\n",
    "    corr = dfnum[valid_cols].corr(numeric_only=True)\n",
    "\n",
    "    plt.figure(figsize=(5 + 1.2*len(valid_cols), 4 + 0.8*len(valid_cols)))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, square=True,\n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(f\"Orbital Parameters Correlation - {name}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "2f132c07f06faace",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _extract_year_any(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Extract year from a numeric or datetime-like series robustly.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
    "        return pd.to_numeric(series, errors=\"coerce\").astype('Int64')\n",
    "    sdt = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    return sdt.dt.year.astype('Int64')\n",
    "\n",
    "\n",
    "debris_year_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if (\"OBJECT_TYPE\" in df.columns) and (\"LAUNCH_YEAR\" in df.columns or \"LAUNCH\" in df.columns):\n",
    "        debris_year_datasets.append(name)\n",
    "\n",
    "print(\"Datasets considered for Debris Count by Launch Year:\", debris_year_datasets)\n",
    "\n",
    "for name in debris_year_datasets:\n",
    "    print(f\"\\nPlotting Debris Count by Launch Year for: {name}\")\n",
    "    df = preprocessed[name]\n",
    "\n",
    "    # Identify debris-like entries by OBJECT_TYPE containing 'DEB'\n",
    "    obj = df[\"OBJECT_TYPE\"].astype(str).str.upper().fillna(\"\")\n",
    "    is_debris = obj.str.contains(\"DEB\")\n",
    "\n",
    "    # Determine launch year\n",
    "    if \"LAUNCH_YEAR\" in df.columns:\n",
    "        years = _extract_year_any(df[\"LAUNCH_YEAR\"]).astype('Int64')\n",
    "    else:\n",
    "        years = _extract_year_any(df[\"LAUNCH\"]).astype('Int64')\n",
    "\n",
    "    debris_years = years[is_debris]\n",
    "    debris_years = debris_years.dropna()\n",
    "\n",
    "    if debris_years.empty:\n",
    "        print(\"  No debris entries with launch year; skipping.\")\n",
    "        continue\n",
    "\n",
    "    counts = debris_years.value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(x=counts.index.astype(int), y=counts.values, color=sns.color_palette()[0])\n",
    "    plt.title(f\"Debris Objects by Launch Year - {name}\")\n",
    "    plt.xlabel(\"Launch Year\")\n",
    "    plt.ylabel(\"Count of Debris Objects\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "332f7eb631108de8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _extract_year_any(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Extract year from a numeric or datetime-like series robustly.\"\"\"\n",
    "    if pd.api.types.is_integer_dtype(series) or pd.api.types.is_float_dtype(series):\n",
    "        return pd.to_numeric(series, errors=\"coerce\").astype('Int64')\n",
    "    sdt = pd.to_datetime(series, errors=\"coerce\", utc=True)\n",
    "    return sdt.dt.year.astype('Int64')\n",
    "\n",
    "\n",
    "debris_year_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if (\"OBJECT_TYPE\" in df.columns) and (\"LAUNCH_YEAR\" in df.columns or \"LAUNCH\" in df.columns):\n",
    "        debris_year_datasets.append(name)\n",
    "\n",
    "print(\"Datasets considered for Debris Count by Launch Year and Country:\", debris_year_datasets)\n",
    "\n",
    "for name in debris_year_datasets:\n",
    "    print(f\"\\nPlotting Debris Counts for: {name}\")\n",
    "    df = preprocessed[name]\n",
    "\n",
    "    # Identify debris-like entries by OBJECT_TYPE containing 'DEB'\n",
    "    obj = df[\"OBJECT_TYPE\"].astype(str).str.upper().fillna(\"\")\n",
    "    is_debris = obj.str.contains(\"DEB\")\n",
    "\n",
    "    # Determine launch year\n",
    "    if \"LAUNCH_YEAR\" in df.columns:\n",
    "        years = _extract_year_any(df[\"LAUNCH_YEAR\"]).astype('Int64')\n",
    "    else:\n",
    "        years = _extract_year_any(df[\"LAUNCH\"]).astype('Int64')\n",
    "\n",
    "    debris_years = years[is_debris]\n",
    "    debris_years = debris_years.dropna()\n",
    "\n",
    "    if debris_years.empty:\n",
    "        print(\"  No debris entries with launch year; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Plot 1: Debris Count by Launch Year ---\n",
    "    counts = debris_years.value_counts().sort_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.barplot(x=counts.index.astype(int), y=counts.values, color=sns.color_palette()[0])\n",
    "    plt.title(f\"Debris Objects by Launch Year - {name}\")\n",
    "    plt.xlabel(\"Launch Year\")\n",
    "    plt.ylabel(\"Count of Debris Objects\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plot 2: Debris Count by Country of Origin Over Time ---\n",
    "    if \"COUNTRY\" in df.columns:\n",
    "        countries = df[\"COUNTRY\"].astype(str).fillna(\"Unknown\")\n",
    "    elif \"COUNTRY_OF_ORIGIN\" in df.columns:\n",
    "        countries = df[\"COUNTRY_OF_ORIGIN\"].astype(str).fillna(\"Unknown\")\n",
    "    else:\n",
    "        print(\"  No country column found; skipping country plot.\")\n",
    "        continue\n",
    "\n",
    "    debris_df = pd.DataFrame({\n",
    "        \"Year\": debris_years,\n",
    "        \"Country\": countries[is_debris]\n",
    "    }).dropna()\n",
    "\n",
    "    if debris_df.empty:\n",
    "        print(\"  No debris entries with country info; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Aggregate counts per year and country\n",
    "    debris_counts = debris_df.groupby([\"Year\", \"Country\"]).size().reset_index(name=\"Count\")\n",
    "\n",
    "    # --- Filter countries by total debris threshold ---\n",
    "    threshold = 200\n",
    "    top_countries = (\n",
    "        debris_counts.groupby(\"Country\")[\"Count\"].sum()\n",
    "        .loc[lambda s: s > threshold].index\n",
    "    )\n",
    "\n",
    "    filtered = debris_counts[debris_counts[\"Country\"].isin(top_countries)]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=filtered, x=\"Year\", y=\"Count\", hue=\"Country\", marker=\"o\")\n",
    "    plt.title(f\"Debris by Country of Origin Over Time (Top countries) - {name}\")\n",
    "    plt.xlabel(\"Launch Year\")\n",
    "    plt.ylabel(\"Count of Debris Objects\")\n",
    "    plt.legend(title=\"Country\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "db58c67aa7676d45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Categorical vs. Numerical Analysis**",
   "id": "14e6477213c46bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _limit_top_with_other(series: pd.Series, top_n: int = 12, other_label: str = \"Other\") -> pd.Series:\n",
    "    s = _clean_categorical(series)\n",
    "    vc = s.value_counts()\n",
    "    if len(vc) <= top_n:\n",
    "        return s\n",
    "    top = set(vc.nlargest(top_n).index)\n",
    "    return s.apply(lambda x: x if x in top else other_label)\n",
    "\n",
    "\n",
    "def _prepare_country_object_type(df: pd.DataFrame, top_countries: int = 12, top_types: int = 12):\n",
    "    if not (\"COUNTRY\" in df.columns and \"OBJECT_TYPE\" in df.columns):\n",
    "        return None\n",
    "    countries = _limit_top_with_other(df[\"COUNTRY\"], top_n=top_countries)\n",
    "    obj_types = _limit_top_with_other(df[\"OBJECT_TYPE\"], top_n=top_types)\n",
    "    ctab = pd.crosstab(countries, obj_types)\n",
    "    if ctab.empty:\n",
    "        return None\n",
    "    # Normalize per country (row-wise) to proportions\n",
    "    ctab_prop = ctab.div(ctab.sum(axis=1), axis=0).fillna(0)\n",
    "    return ctab, ctab_prop\n",
    "\n",
    "\n",
    "def plot_country_vs_object_type_stacked(ax, ctab_prop: pd.DataFrame, title: str):\n",
    "    # Use a consistent palette across object types\n",
    "    categories = list(ctab_prop.columns)\n",
    "    palette = sns.color_palette(n_colors=len(categories))\n",
    "    bottom = np.zeros(len(ctab_prop))\n",
    "    x = np.arange(len(ctab_prop))\n",
    "    for i, cat in enumerate(categories):\n",
    "        vals = ctab_prop[cat].values\n",
    "        ax.bar(x, vals, bottom=bottom, color=palette[i], label=cat)\n",
    "        bottom += vals\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ctab_prop.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=\"Object Type\", bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "\n",
    "\n",
    "country_objtype_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if (\"COUNTRY\" in df.columns) and (\"OBJECT_TYPE\" in df.columns):\n",
    "        country_objtype_datasets.append(name)\n",
    "\n",
    "print(\"Datasets considered for Country vs. Object Type:\", country_objtype_datasets)\n",
    "\n",
    "for name in country_objtype_datasets:\n",
    "    print(f\"\\nPlotting Country vs. Object Type (stacked) for: {name}\")\n",
    "    df = preprocessed[name]\n",
    "    prepared = _prepare_country_object_type(df, top_countries=12, top_types=10)\n",
    "    if prepared is None:\n",
    "        print(\"  Not enough data; skipping.\")\n",
    "        continue\n",
    "    ctab, ctab_prop = prepared\n",
    "    if ctab_prop.shape[0] == 0 or ctab_prop.shape[1] == 0:\n",
    "        print(\"  Empty contingency after processing; skipping.\")\n",
    "        continue\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(max(8, 0.7*len(ctab_prop.index)+4), 5))\n",
    "    plot_country_vs_object_type_stacked(ax, ctab_prop, title=f\"Object Type Distribution by Country - {name}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "bdc908d4a8f7a12f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "orbital_regime_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    needed = {\"COUNTRY\", \"PERIGEE\", \"APOGEE\"}\n",
    "    if needed.issubset(df.columns):\n",
    "        orbital_regime_datasets.append(name)\n",
    "\n",
    "print(\"Datasets considered for Country vs. Orbital Regimes:\", orbital_regime_datasets)\n",
    "\n",
    "for name in orbital_regime_datasets:\n",
    "    print(f\"\\nPlotting Apogee vs. Perigee by Country for: {name}\")\n",
    "    df = preprocessed[name].copy()\n",
    "    # Coerce numeric and drop invalid/negative altitudes\n",
    "    perigee = pd.to_numeric(df[\"PERIGEE\"], errors=\"coerce\")\n",
    "    apogee = pd.to_numeric(df[\"APOGEE\"], errors=\"coerce\")\n",
    "    valid = perigee.notna() & apogee.notna() & (perigee >= 0) & (apogee >= 0)\n",
    "    df = df.loc[valid, [\"COUNTRY\", \"PERIGEE\", \"APOGEE\"]].copy()\n",
    "    if df.empty:\n",
    "        print(\"  No valid apogee/perigee data; skipping.\")\n",
    "        continue\n",
    "    df[\"COUNTRY_LIM\"] = _limit_top_with_other(df[\"COUNTRY\"], top_n=10)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        data=df,\n",
    "        x=\"PERIGEE\",\n",
    "        y=\"APOGEE\",\n",
    "        hue=\"COUNTRY\",\n",
    "        s=12,\n",
    "        alpha=0.5,\n",
    "        edgecolor=None,\n",
    "    )\n",
    "    plt.title(f\"Apogee vs. Perigee by Country - {name}\")\n",
    "    plt.xlabel(\"Perigee altitude (km)\")\n",
    "    plt.ylabel(\"Apogee altitude (km)\")\n",
    "    plt.legend(title=\"Country\", bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "602bedb51413e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rcs_size_datasets = []\n",
    "for name, df in preprocessed.items():\n",
    "    if (\"OBJECT_TYPE\" in df.columns) and (\"RCS_SIZE\" in df.columns):\n",
    "        rcs_size_datasets.append(name)\n",
    "\n",
    "print(\"Datasets considered for Object Type vs. RCS_SIZE (stacked):\", rcs_size_datasets)\n",
    "\n",
    "for name in rcs_size_datasets:\n",
    "    print(f\"\\nPlotting RCS_SIZE distribution by Object Type for: {name}\")\n",
    "    df = preprocessed[name].copy()\n",
    "    sub = df[[\"OBJECT_TYPE\", \"RCS_SIZE\"]].dropna()\n",
    "    if sub.empty:\n",
    "        print(\"  No RCS_SIZE data; skipping.\")\n",
    "        continue\n",
    "    sub[\"OBJECT_TYPE_LIM\"] = _limit_top_with_other(sub[\"OBJECT_TYPE\"], top_n=12)\n",
    "    sub[\"RCS_SIZE_CLEAN\"] = _clean_categorical(sub[\"RCS_SIZE\"])\n",
    "\n",
    "    ctab = pd.crosstab(sub[\"OBJECT_TYPE_LIM\"], sub[\"RCS_SIZE_CLEAN\"])\n",
    "    if ctab.empty:\n",
    "        print(\"  Empty contingency; skipping.\")\n",
    "        continue\n",
    "    ctab_prop = ctab.div(ctab.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(max(8, 0.7*len(ctab_prop.index)+4), 5))\n",
    "    categories = list(ctab_prop.columns)\n",
    "    palette = sns.color_palette(n_colors=len(categories))\n",
    "    bottom = np.zeros(len(ctab_prop))\n",
    "    x = np.arange(len(ctab_prop))\n",
    "    for i, cat in enumerate(categories):\n",
    "        vals = ctab_prop[cat].values\n",
    "        ax.bar(x, vals, bottom=bottom, color=palette[i], label=cat)\n",
    "        bottom += vals\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ctab_prop.index, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Proportion\")\n",
    "    ax.set_title(f\"RCS Size Distribution by Object Type - {name}\")\n",
    "    ax.legend(title=\"RCS Size\", bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "ef72839558b8b3e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Conjunction and Risk Analysis (spacetrack_cdm_public)**",
   "id": "674b197f4c8e78a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "high_risk_df = None\n",
    "def find_col(df, candidates):\n",
    "    \"\"\"Finds the first matching column name from a list of candidates, case-insensitively.\"\"\"\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in cols:\n",
    "            return cols[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def analyze_high_risk_conjunctions(\n",
    "    cdm_df: pd.DataFrame,\n",
    "    pc_threshold: float = None,\n",
    "    rng_threshold_km: float = None,\n",
    "    use_suggested_thresholds: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs a comprehensive analysis of high-risk conjunctions in a CDM dataset.\n",
    "\n",
    "    This function identifies high-risk events using adaptive or manual thresholds,\n",
    "    displays key statistics, and generates insightful visualizations.\n",
    "\n",
    "    Args:\n",
    "        cdm_df (pd.DataFrame): The input DataFrame containing conjunction data.\n",
    "        pc_threshold (float, optional): Manual threshold for Probability of Collision (PC).\n",
    "        rng_threshold_km (float, optional): Manual threshold for Minimum Range in km.\n",
    "        use_suggested_thresholds (bool): If True, automatically uses data-driven\n",
    "            percentiles as thresholds if manual ones aren't provided. This is highly\n",
    "            recommended to adapt to the dataset's specific distribution.\n",
    "    \"\"\"\n",
    "    if cdm_df is None or cdm_df.empty:\n",
    "        print(\"CDM DataFrame is empty. Cannot perform analysis.\")\n",
    "        return\n",
    "\n",
    "    # 1. --- Column Identification and Data Preparation ---\n",
    "    pc_col = find_col(cdm_df, [\"PC\", \"PROBABILITY_OF_COLLISION\"])\n",
    "    rng_col = find_col(cdm_df, [\"MIN_RNG\", \"MISS_DISTANCE_KM\", \"DCA\"])\n",
    "    tca_col = find_col(cdm_df, [\"TCA\", \"TIME_OF_CLOSEST_APPROACH\"])\n",
    "\n",
    "    # Coerce to numeric and datetime, handling potential errors\n",
    "    if pc_col:\n",
    "        cdm_df[pc_col] = pd.to_numeric(cdm_df[pc_col], errors=\"coerce\")\n",
    "    if rng_col:\n",
    "        cdm_df[rng_col] = pd.to_numeric(cdm_df[rng_col], errors=\"coerce\")\n",
    "    if tca_col:\n",
    "        cdm_df[tca_col] = pd.to_datetime(cdm_df[tca_col], errors=\"coerce\", utc=True)\n",
    "\n",
    "\n",
    "    print(\"--- Conjunction Risk Analysis ---\")\n",
    "\n",
    "    # 2. --- Adaptive Threshold Calculation ---\n",
    "    s_pc = cdm_df[pc_col].dropna()\n",
    "    s_rng = cdm_df[rng_col].dropna()\n",
    "\n",
    "    if use_suggested_thresholds:\n",
    "        # If no manual threshold is given, suggest and use a percentile-based one\n",
    "        if pc_threshold is None and not s_pc.empty:\n",
    "            pc_threshold = float(s_pc.quantile(0.99)) # Top 1% of collision probabilities\n",
    "            print(f\"📈 Using suggested PC threshold (99th percentile): {pc_threshold:.2e}\")\n",
    "        if rng_threshold_km is None and not s_rng.empty:\n",
    "            rng_threshold_km = float(s_rng.quantile(0.05)) # Closest 5% of approaches\n",
    "            print(f\"📉 Using suggested MIN_RNG threshold (5th percentile): {rng_threshold_km:.3f} km\")\n",
    "\n",
    "    # Fallback to hardcoded defaults if suggestions can't be computed\n",
    "    HR_PC_THRESHOLD = pc_threshold if pc_threshold is not None else 1e-5\n",
    "    HR_MINRNG_THRESHOLD_KM = rng_threshold_km if rng_threshold_km is not None else 0.5\n",
    "\n",
    "    print(f\"\\nFinal Thresholds Used:\")\n",
    "    print(f\"  - High PC >= {HR_PC_THRESHOLD:.2e}\")\n",
    "    print(f\"  - Small MIN_RNG <= {HR_MINRNG_THRESHOLD_KM:.3f} km\")\n",
    "\n",
    "    # 3. --- High-Risk Filtering (using AND logic) ---\n",
    "    mask_pc = cdm_df[pc_col] >= HR_PC_THRESHOLD\n",
    "    mask_rng = cdm_df[rng_col] <= HR_MINRNG_THRESHOLD_KM\n",
    "    high_risk_mask = mask_pc & mask_rng # Intersection is more robust for \"high-risk\"\n",
    "\n",
    "    high_risk = cdm_df.loc[high_risk_mask].copy()\n",
    "\n",
    "    print(f\"\\n✅ Found {len(high_risk)} high-risk events out of {len(cdm_df)} total.\")\n",
    "\n",
    "    if len(high_risk) == 0:\n",
    "        print(\"No events met the specified criteria. Consider relaxing thresholds or checking data.\")\n",
    "        return\n",
    "\n",
    "    # 4. --- Display High-Risk Events Table ---\n",
    "    cols_to_show = [c for c in [\n",
    "        tca_col, pc_col, rng_col,\n",
    "        find_col(cdm_df, [\"SAT_1_NAME\"]), find_col(cdm_df, [\"SAT_2_NAME\"]),\n",
    "        find_col(cdm_df, [\"REL_SPEED\"]), find_col(cdm_df, [\"SAT_1_ID\"]), find_col(cdm_df, [\"SAT_2_ID\"])\n",
    "    ] if c is not None]\n",
    "\n",
    "    print(\"\\n📋 Sample of High-Risk Events:\")\n",
    "    display(high_risk[cols_to_show].sort_values(by=pc_col, ascending=False).head(10).style.format({pc_col: \"{:.2e}\"}))\n",
    "\n",
    "    result = high_risk[cols_to_show].sort_values(by=pc_col, ascending=False)\n",
    "\n",
    "    # 5. --- Visualizations with Threshold Context ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    fig.suptitle('High-Risk Conjunction Analysis', fontsize=16)\n",
    "\n",
    "    # PC Distribution\n",
    "    sns.kdeplot(s_pc, ax=axes[0], label='All Events', color='gray', log_scale=True)\n",
    "    if not high_risk[pc_col].dropna().empty:\n",
    "        sns.kdeplot(high_risk[pc_col].dropna(), ax=axes[0], label='High-Risk', color='red', log_scale=True)\n",
    "    axes[0].axvline(HR_PC_THRESHOLD, color='r', linestyle='--', label=f'Threshold ({HR_PC_THRESHOLD:.1e})')\n",
    "    axes[0].set_title('PC Distribution (Log Scale)')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # MIN_RNG Distribution\n",
    "    sns.kdeplot(s_rng, ax=axes[1], label='All Events', color='gray')\n",
    "    if not high_risk[rng_col].dropna().empty:\n",
    "        sns.kdeplot(high_risk[rng_col].dropna(), ax=axes[1], label='High-Risk', color='red')\n",
    "    axes[1].axvline(HR_MINRNG_THRESHOLD_KM, color='r', linestyle='--', label=f'Threshold ({HR_MINRNG_THRESHOLD_KM:.2f} km)')\n",
    "    axes[1].set_title('MIN_RNG Distribution (km)')\n",
    "    axes[1].set_xlim(left=0, right=max(s_rng.quantile(0.95), HR_MINRNG_THRESHOLD_KM * 2)) # Zoom in on relevant range\n",
    "    axes[1].legend()\n",
    "\n",
    "    # PC vs MIN_RNG Scatter Plot\n",
    "    sns.scatterplot(data=cdm_df, x=rng_col, y=pc_col, ax=axes[2], color='gray', alpha=0.2, s=10, label='All Events')\n",
    "    sns.scatterplot(data=high_risk, x=rng_col, y=pc_col, ax=axes[2], color='red', s=40, edgecolor='k', label='High-Risk')\n",
    "    axes[2].set_yscale('log')\n",
    "    axes[2].axhline(HR_PC_THRESHOLD, color='r', linestyle='--')\n",
    "    axes[2].axvline(HR_MINRNG_THRESHOLD_KM, color='r', linestyle='--')\n",
    "    axes[2].set_title('PC vs. MIN_RNG (High-Risk Quadrant)')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    # 6. --- Operational Urgency: Time-to-TCA Analysis ---\n",
    "    if tca_col and pd.Timestamp.now(tz='utc'):\n",
    "        now = pd.Timestamp.now(tz='utc')\n",
    "        high_risk['TIME_TO_TCA_H'] = (high_risk[tca_col] - now) / pd.Timedelta(hours=1)\n",
    "\n",
    "        # Filter for future events\n",
    "        future_events = high_risk[high_risk['TIME_TO_TCA_H'] > 0]\n",
    "\n",
    "        if not future_events.empty:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            bins = [0, 24, 48, 72, 168, future_events['TIME_TO_TCA_H'].max()]\n",
    "            labels = ['<24h', '24-48h', '48-72h', '3-7 days', '>7 days']\n",
    "\n",
    "            time_dist = pd.cut(future_events['TIME_TO_TCA_H'], bins=bins, labels=labels, right=False).value_counts().sort_index()\n",
    "\n",
    "            sns.barplot(x=time_dist.index, y=time_dist.values)\n",
    "            plt.title('Operational Urgency: When High-Risk Events Occur')\n",
    "            plt.xlabel('Time to Closest Approach (TCA)')\n",
    "            plt.ylabel('Number of High-Risk Events')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return result\n",
    "\n",
    "cdm_df = preprocessed['spacetrack_cdm_public_30d']\n",
    "# Run the analysis with adaptive thresholds (recommended)\n",
    "high_risk_df = analyze_high_risk_conjunctions(cdm_df)"
   ],
   "id": "c6ed3df1673a02f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Ploting High risk conjunctions in 3D**",
   "id": "4a2588025a385348"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "high_risk_sample = high_risk_df.iloc[0]\n",
    "Sat_1_id = high_risk_sample[\"SAT_1_ID\"]\n",
    "Sat_2_id = high_risk_sample[\"SAT_2_ID\"]\n",
    "tca = high_risk_sample[\"TCA\"]\n",
    "epoch = str(tca).split(\" \")[0]\n",
    "epoch_end = str(tca + timedelta(days=1)).split(\" \")[0]\n"
   ],
   "id": "5a311463d5c42980",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    with space_track_client() as st:\n",
    "        sat_1_tle = st.fetch_tle_by_id_and_epoch(Sat_1_id, epoch_start=epoch, epoch_end=epoch_end)\n",
    "        sat_2_tle = st.fetch_tle_by_id_and_epoch(Sat_1_id, epoch_start=epoch, epoch_end=epoch_end)\n",
    "except SpaceTrackAuthError as e:\n",
    "    print(\"Space-Track credentials not set or login failed:\", e)\n",
    "\n",
    "l1_sat_1 = sat_1_tle[\"TLE_LINE1\"][0]\n",
    "l2_sat_1 = sat_1_tle[\"TLE_LINE2\"][0]\n",
    "l1_sat_2 = sat_2_tle[\"TLE_LINE1\"][0]\n",
    "l2_sat_2 = sat_2_tle[\"TLE_LINE2\"][0]"
   ],
   "id": "338f0e03ed054ab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compare_orbits(l1_sat_1, l2_sat_1, l1_sat_2, l2_sat_2, str(Sat_1_id), str(Sat_2_id))",
   "id": "5922c5811b06c90d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
